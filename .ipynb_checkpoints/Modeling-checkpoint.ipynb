{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126db5b6",
   "metadata": {},
   "source": [
    "# Modeling #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c316c2a0",
   "metadata": {},
   "source": [
    "## Import APIs ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89b05c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import wfdb\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7621688",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1acd02",
   "metadata": {},
   "source": [
    "## Load data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b55a9af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptbxl_data = pd.read_csv('./cleaned_data/cleaned_ptbxl_metadata.csv', index_col='ecg_id')\n",
    "normal_data = ptbxl_df[ptbxl_df['diagnostic_superclass'] == 'NORMAL']\n",
    "abnormal_data = ptbxl_df[ptbxl_df['diagnostic_superclass'] == 'ABNORMAL']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd8d169",
   "metadata": {},
   "source": [
    "### Metadata ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2bad2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal metadata shape: (9069, 4)\n",
      "Abnormal metadata shape: (9069, 4)\n"
     ]
    }
   ],
   "source": [
    "normal_metadata = normal_data.loc[:, ['age', 'sex', 'device', 'validated_by_human']].copy()\n",
    "abnormal_metadata = abnormal_data.loc[:, ['age', 'sex', 'device', 'validated_by_human']].copy()\n",
    "print(f'Normal metadata shape: {normal_metadata.shape}')\n",
    "print(f'Abnormal metadata shape: {abnormal_metadata.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4c27e",
   "metadata": {},
   "source": [
    "### ECG Waveform data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23d07cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_ecg_data(data):\n",
    "    new_data = []\n",
    "    for idx in data.index:\n",
    "        record_path = data.loc[idx]['filename_hr']\n",
    "        waveform_df = pd.read_csv('./cleaned_data/waveform_data/' + record_path + '.csv', index_col='Time (s)')\n",
    "        new_data.append(waveform_df)\n",
    "    new_data = np.array(new_data)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff7c522c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal ECG data shape: (9069, 1000, 12)\n",
      "Abnormal ECG data shape: (9069, 1000, 12)\n"
     ]
    }
   ],
   "source": [
    "normal_ecg_data = extract_ecg_data(normal_data)\n",
    "abnormal_ecg_data = extract_ecg_data(abnormal_data)\n",
    "print(f'Normal ECG data shape: {normal_ecg_data.shape}')\n",
    "print(f'Abnormal ECG data shape: {abnormal_ecg_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a322ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# waveform_data = np.load(\"./cleaned_data/waveform_np.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760bf66",
   "metadata": {},
   "source": [
    "## Train-test split ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ca5dc",
   "metadata": {},
   "source": [
    "This recommended train-test split code was obtained from the downloaded folder with the dataset: https://physionet.org/content/ptb-xl/1.0.3/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11c74a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal ECG train shape: (7255, 1000, 12)\n",
      "Normal ECG test shape: (1814, 1000, 12)\n",
      "Abnormal ECG train shape: (7255, 1000, 12)\n",
      "Abnormal ECG test shape: (1814, 1000, 12)\n",
      "Normal metadata train shape: (7255, 4)\n",
      "Normal metadata test shape: (1814, 4)\n",
      "Abnormal metadata train shape: (7255, 4)\n",
      "Abnormal metadata test shape: (1814, 4)\n"
     ]
    }
   ],
   "source": [
    "split_idx = int(normal_data.shape[0] * 0.8)\n",
    "\n",
    "normal_ecg_train = normal_ecg_data[0:split_idx]\n",
    "normal_ecg_test = normal_ecg_data[split_idx:]\n",
    "print(f'Normal ECG train shape: {normal_ecg_train.shape}')\n",
    "print(f'Normal ECG test shape: {normal_ecg_test.shape}')\n",
    "\n",
    "abnormal_ecg_train = abnormal_ecg_data[0:split_idx]\n",
    "abnormal_ecg_test = abnormal_ecg_data[split_idx:]\n",
    "print(f'Abnormal ECG train shape: {abnormal_ecg_train.shape}')\n",
    "print(f'Abnormal ECG test shape: {abnormal_ecg_test.shape}')\n",
    "\n",
    "normal_metadata_train = normal_metadata[0:split_idx]\n",
    "normal_metadata_test = normal_metadata[split_idx:]\n",
    "print(f'Normal metadata train shape: {normal_metadata_train.shape}')\n",
    "print(f'Normal metadata test shape: {normal_metadata_test.shape}')\n",
    "\n",
    "abnormal_metadata_train = abnormal_metadata[0:split_idx]\n",
    "abnormal_metadata_test = abnormal_metadata[split_idx:]\n",
    "print(f'Abnormal metadata train shape: {abnormal_metadata_train.shape}')\n",
    "print(f'Abnormal metadata test shape: {abnormal_metadata_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c04c2b",
   "metadata": {},
   "source": [
    "## Normalize ECG waveform data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6065566c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_waveform(data):\n",
    "    # Code generated from Bing Copilot\n",
    "    normalized_data = np.empty_like(data)\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[2]):\n",
    "            min_val = np.min(data[i, :, j])\n",
    "            max_val = np.max(data[i, :, j])\n",
    "\n",
    "            if max_val == min_val:\n",
    "                normalized_data[i, :, j] = 0\n",
    "            else:\n",
    "                normalized_data[i, :, j] = (data[i, :, j] - min_val) / (max_val - min_val)\n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "344585ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Since normalization occurs only within each record, there will be no contamination from train data\n",
    "std_normal_ecg_train = normalize_waveform(normal_ecg_train)\n",
    "std_normal_ecg_test = normalize_waveform(normal_ecg_test)\n",
    "\n",
    "std_abnormal_ecg_train = normalize_waveform(abnormal_ecg_train)\n",
    "std_abnormal_ecg_test = normalize_waveform(abnormal_ecg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf9b501",
   "metadata": {},
   "source": [
    "## Normalize and one-hot encode metadata ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70a94634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generated from ChatGPT\n",
    "scaler_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "encoder_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "normal_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler_transformer, ['age']),\n",
    "        ('cat', encoder_transformer, ['sex', 'device', 'validated_by_human'])\n",
    "    ])\n",
    "\n",
    "std_normal_metadata_train = normal_preprocessor.fit_transform(normal_metadata_train).toarray()\n",
    "std_normal_metadata_test = normal_preprocessor.transform(normal_metadata_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ef6c16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "encoder_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "abnormal_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler_transformer, ['age']),\n",
    "        ('cat', encoder_transformer, ['sex', 'device', 'validated_by_human'])\n",
    "    ])\n",
    "\n",
    "std_abnormal_metadata_train = abnormal_preprocessor.fit_transform(abnormal_metadata_train).toarray()\n",
    "std_abnormal_metadata_test = abnormal_preprocessor.transform(abnormal_metadata_test).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbdb9b3",
   "metadata": {},
   "source": [
    "## Initialize Autoencoder Dataloaders ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6f16bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_ecg_train_set = torch.from_numpy(std_normal_ecg_train).float()\n",
    "autoencoder_metadata_train_set = torch.from_numpy(std_normal_metadata_train).float()\n",
    "\n",
    "normal_ecg_test_set = torch.from_numpy(std_normal_ecg_test).float()\n",
    "normal_metadata_test_set = torch.from_numpy(std_normal_metadata_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd869284",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "autoencoder_train_dataset = TensorDataset(autoencoder_ecg_train_set, autoencoder_metadata_train_set)\n",
    "autoencoder_train_loader = DataLoader(autoencoder_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "autoencoder_test_dataset = TensorDataset(normal_ecg_test_set, normal_metadata_test_set)\n",
    "autoencoder_test_loader = DataLoader(autoencoder_test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976810d9",
   "metadata": {},
   "source": [
    "## CNN autoencoder + LSTM metadata model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47e5382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=12,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=4,\n",
    "                      stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(in_channels=64,\n",
    "                               out_channels=12,\n",
    "                               kernel_size=4,\n",
    "                               stride=1),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Upsample(1000)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        encoded_output = self.encoder(x)\n",
    "        decoded_output = self.decoder(encoded_output)\n",
    "        return decoded_output.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60308ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetadataLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(16, 64, batch_first=True)\n",
    "        self.output_layer = nn.Linear(64, 12)\n",
    "    \n",
    "    def forward(self, x, h):\n",
    "        lstm_outputs, h_n = self.lstm(x, h)\n",
    "        outputs = self.output_layer(lstm_outputs.squeeze(dim=1))\n",
    "        return outputs, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "834c1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, cnn_autoencoder, lstm_model):\n",
    "        super().__init__()\n",
    "        self.cnn_autoencoder = cnn_autoencoder\n",
    "        self.lstm_model = lstm_model\n",
    "        self.fc = nn.Linear(24, 12)\n",
    "        \n",
    "    def forward(self, ecg_data, metadata, hc):\n",
    "        cnn_output = self.cnn_autoencoder(ecg_data)\n",
    "        lstm_output, hc_n = self.lstm_model(metadata, hc)\n",
    "        reshaped_lstm_output = lstm_output.unsqueeze(1).repeat(1, 1000, 1)\n",
    "        combined_output = torch.cat((cnn_output, reshaped_lstm_output), dim=-1)\n",
    "        output = self.fc(combined_output)\n",
    "\n",
    "        return output, hc_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f47c0",
   "metadata": {},
   "source": [
    "### Training ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "744f0179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (cnn_autoencoder): CNNAutoencoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv1d(12, 64, kernel_size=(4,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): ConvTranspose1d(64, 12, kernel_size=(4,), stride=(1,))\n",
       "      (1): Sigmoid()\n",
       "      (2): Upsample(size=1000, mode='nearest')\n",
       "    )\n",
       "  )\n",
       "  (lstm_model): MetadataLSTM(\n",
       "    (lstm): LSTM(16, 64, batch_first=True)\n",
       "    (output_layer): Linear(in_features=64, out_features=12, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=24, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_autoencoder_model = CNNAutoencoder()\n",
    "lstm_metadata_model = MetadataLSTM()\n",
    "\n",
    "combinedModel = CombinedModel(cnn_autoencoder_model, lstm_metadata_model)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(combinedModel.parameters(), lr=1e-3)\n",
    "\n",
    "combinedModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e3040d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_combined(model, train_loader, criterion, optimizer, nepoch=10):\n",
    "    for epoch in range(nepoch):\n",
    "        num_layers = model.lstm_model.lstm.num_layers\n",
    "        hidden_size = model.lstm_model.lstm.hidden_size\n",
    "        h_0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "        c_0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "        for batch_ecg, batch_metadata in train_loader:\n",
    "            ecg_data = batch_ecg.to(device).float()\n",
    "            metadata = batch_metadata.to(device).float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_data, (h_n, c_n) = model(ecg_data, metadata, (h_0, c_0))\n",
    "            loss = criterion(reconstructed_data, ecg_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{nepoch}], Loss: {loss.item()}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d463c1bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.009302345104515553\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_combined(combinedModel, autoencoder_train_loader, criterion, optimizer)\n",
      "Cell \u001b[0;32mIn[33], line 12\u001b[0m, in \u001b[0;36mtrain_combined\u001b[0;34m(model, train_loader, criterion, optimizer, nepoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m metadata \u001b[38;5;241m=\u001b[39m batch_metadata\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m reconstructed_data, (h_n, c_n) \u001b[38;5;241m=\u001b[39m model(ecg_data, metadata, (h_0, c_0))\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed_data, ecg_data)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m, in \u001b[0;36mCombinedModel.forward\u001b[0;34m(self, ecg_data, metadata, hc)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ecg_data, metadata, hc):\n\u001b[0;32m----> 9\u001b[0m     cnn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcnn_autoencoder(ecg_data)\n\u001b[1;32m     10\u001b[0m     lstm_output, hc_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm_model(metadata, hc)\n\u001b[1;32m     11\u001b[0m     reshaped_lstm_output \u001b[38;5;241m=\u001b[39m lstm_output\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 22\u001b[0m, in \u001b[0;36mCNNAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     encoded_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     23\u001b[0m     decoded_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoded_output)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoded_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:310\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:306\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    304\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    305\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    307\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_combined(combinedModel, autoencoder_train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(combinedModel.state_dict(), \"./models/combinedModel.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1776046",
   "metadata": {},
   "source": [
    "### Visualization ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, ecg_input, metadata_input):\n",
    "    lead_vals = ['I', 'II', 'III', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']\n",
    "    time = np.arange(0, 1000) / 100\n",
    "    \n",
    "    num_layers = model.lstm_model.lstm.num_layers\n",
    "    hidden_size = model.lstm_model.lstm.hidden_size\n",
    "    h_0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "    c_0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "    \n",
    "    prediction, _ = model(ecg_input, metadata_input, (h_0, c_0))\n",
    "    fig, axs = plt.subplots(6, 2, figsize=(15, 20))\n",
    "    for i in range(6):\n",
    "        for j in range(2):\n",
    "            lead_idx = i * 2 + j\n",
    "            lead_val = lead_vals[lead_idx]\n",
    "            \n",
    "            original_ecg = ecg_input[0, :, lead_idx].detach().numpy()\n",
    "            reconstructed_ecg = prediction[0, :, lead_idx].detach().numpy()\n",
    "            \n",
    "            axs[i, j].plot(time, original_ecg, label='Original')\n",
    "            axs[i, j].plot(time, reconstructed_ecg, label='Reconstructed')\n",
    "            axs[i, j].set_xlabel('Time (s)')\n",
    "            axs[i, j].set_ylabel('Normalized ECG Reading')\n",
    "            axs[i, j].set_title(f'Lead {lead_val}')\n",
    "            axs[i, j].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f90630",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_data, batch_metadata in test_loader:\n",
    "    ecg_data = batch_data.to(device).float()\n",
    "    batch_size = ecg_data.size(0)\n",
    "    batch_metadata = batch_metadata.to(device).float()\n",
    "    \n",
    "    single_ecg_data = ecg_data[0].unsqueeze(0)\n",
    "    single_metadata = batch_metadata[0].unsqueeze(0)\n",
    "    \n",
    "    break\n",
    "\n",
    "visualize_predictions(combinedModel, single_ecg_data, single_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce3631",
   "metadata": {},
   "source": [
    "# Computational Experiment #1: Transfer Learning #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478a54bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392bbb1a",
   "metadata": {},
   "source": [
    "# Computational Experiment #2:  TCN Autoencoder #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc14bb6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb809b3d",
   "metadata": {},
   "source": [
    "Model Card for the Hybrid Autoencoder Model Name: Hybrid Autoencoder for ECG and Metadata\n",
    "\n",
    "Description: This model is designed to learn compressed representations of combined ECG waveform and patient metadata. It utilizes separate pathways for waveform data and metadata, merging them into a dense representation which is then used to reconstruct both types of data.\n",
    "\n",
    "Model Architecture:\n",
    "\n",
    "Waveform Pathway: Convolutional layers followed by pooling and flattening. Metadata Pathway: Dense layers. Combined Encoding and Decoding: Dense layers. Intended Use: Intended for anomaly detection in ECG data where additional patient metadata is available and considered relevant.\n",
    "\n",
    "Data Used for Training: Assumes a dataset comprising ECG waveform data aligned with patient metadata such as age, sex, and device information.\n",
    "\n",
    "Limitations: The model's effectiveness is highly dependent on the quality and preprocessing of the input data. The architecture needs fine-tuning and validation using real-world data to ensure robustness.\n",
    "\n",
    "Ethical Considerations: Care should be taken to avoid biases that may arise from imbalanced data across different demographic groups. Privacy concerns should be addressed when handling patient data.\n",
    "\n",
    "This framework sets up the foundation of your model; further tuning, training, and validation steps are needed to adapt it to specific tasks or datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f9a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_devices = metadata['device'].nunique()\n",
    "print(f\"Number of unique devices: {num_unique_devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tcn import TCN\n",
    "\n",
    "class TCNAutoencoder(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, dropout, metadata_dims):\n",
    "        super(TCNAutoencoder, self).__init__()\n",
    "        self.encoder = TCN(\n",
    "            num_inputs=num_inputs,\n",
    "            num_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            causal=True,\n",
    "        )\n",
    "        self.age_embedding = nn.Linear(1, metadata_dims[0])  # Age is a single value\n",
    "        self.sex_embedding = nn.Linear(2, metadata_dims[1])  # Sex is one-hot encoded (2 columns)\n",
    "        self.device_embedding = nn.Linear(num_unique_devices, metadata_dims[2]) #one hot (11 cols)\n",
    "        self.validated_embedding = nn.Linear(2, metadata_dims[3]) #one hot (2 cols)\n",
    "        \n",
    "        decoder_input_dim = num_channels[-1] + sum(metadata_dims)\n",
    "        self.decoder = TCN(\n",
    "            num_inputs=decoder_input_dim,\n",
    "            num_channels=num_channels[::-1],\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,    \n",
    "            causal=True,\n",
    "            output_projection=num_inputs,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, metadata):\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        age_emb = self.age_embedding(metadata[:, 0].unsqueeze(1))\n",
    "        sex_emb = self.sex_embedding(metadata[:, 1:3])\n",
    "        device_emb = self.device_embedding(metadata[:, 3:-2])\n",
    "        validated_emb = self.validated_embedding(metadata[:, -2:])\n",
    "        \n",
    "        metadata_emb = torch.cat([age_emb, sex_emb, device_emb, validated_emb], dim=-1)\n",
    "        metadata_emb = metadata_emb.unsqueeze(2).expand(-1, -1, encoded.size(2))\n",
    "        \n",
    "        concatenated = torch.cat([encoded, metadata_emb], dim=1)\n",
    "        decoded = self.decoder(concatenated)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e042cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_inputs = 12  # Assuming 12 input channels in the ECG data\n",
    "num_channels = [32, 64, 128]  # Example number of channels in each residual block of the encoder\n",
    "kernel_size = 3  # Example kernel size for the TCN layers\n",
    "dropout = 0.2  # Example dropout rate\n",
    "metadata_dims = [10, 5, 20, 5]  # Example embedding dimensions for age, sex, and device, and validated\n",
    "\n",
    "num_unique_devices = metadata['device'].nunique()\n",
    "print(normalized_metadata_train.shape)\n",
    "assert num_unique_devices == normalized_metadata_train.shape[1] - 5, \"Number of unique devices should match the number of device columns in metadata\"\n",
    "\n",
    "model = TCNAutoencoder(num_inputs, num_channels, kernel_size, dropout, metadata_dims)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(normalized_waveform_train).float(), torch.from_numpy(normalized_metadata_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_data, batch_metadata in train_loader:\n",
    "        ecg_data = batch_data.to(device).permute(0, 2, 1).float()\n",
    "        batch_metadata = batch_metadata.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_data = model(ecg_data, batch_metadata)\n",
    "        loss = criterion(reconstructed_data, ecg_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71fc6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/tcn.pth\")\n",
    "print(f\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure outputs are 1k x 12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
