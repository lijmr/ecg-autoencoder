{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Experiment:  transfer learning our model to a supervised classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import wfdb\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: keep labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal metadata shape: (9069, 5)\n",
      "Abnormal metadata shape: (9069, 5)\n"
     ]
    }
   ],
   "source": [
    "ptbxl_data = pd.read_csv('./cleaned_data/cleaned_ptbxl_metadata.csv', index_col='ecg_id')\n",
    "normal_data = ptbxl_data[ptbxl_data['diagnostic_superclass'] == 'NORMAL']\n",
    "abnormal_data = ptbxl_data[ptbxl_data['diagnostic_superclass'] == 'ABNORMAL']\n",
    "\n",
    "#add \n",
    "normal_metadata = normal_data.loc[:, ['age', 'sex', 'device', 'validated_by_human', 'diagnostic_superclass']].copy()\n",
    "abnormal_metadata = abnormal_data.loc[:, ['age', 'sex', 'device', 'validated_by_human', 'diagnostic_superclass']].copy()\n",
    "print(f'Normal metadata shape: {normal_metadata.shape}')\n",
    "print(f'Abnormal metadata shape: {abnormal_metadata.shape}')\n",
    "\n",
    "normal_ecg_data = np.load(\"./cleaned_data/normal_ecg_data.npy\")\n",
    "abnormal_ecg_data = np.load(\"./cleaned_data/abnormal_ecg_data.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal ECG train shape: (7255, 1000, 12)\n",
      "Normal ECG test shape: (1814, 1000, 12)\n",
      "Abnormal ECG train shape: (7255, 1000, 12)\n",
      "Abnormal ECG test shape: (1814, 1000, 12)\n",
      "Normal metadata train shape: (7255, 5)\n",
      "Normal metadata test shape: (1814, 5)\n",
      "Abnormal metadata train shape: (7255, 5)\n",
      "Abnormal metadata test shape: (1814, 5)\n"
     ]
    }
   ],
   "source": [
    "#train test\n",
    "split_idx = int(normal_data.shape[0] * 0.8)\n",
    "\n",
    "normal_ecg_train = normal_ecg_data[0:split_idx]\n",
    "normal_ecg_test = normal_ecg_data[split_idx:]\n",
    "print(f'Normal ECG train shape: {normal_ecg_train.shape}')\n",
    "print(f'Normal ECG test shape: {normal_ecg_test.shape}')\n",
    "\n",
    "abnormal_ecg_train = abnormal_ecg_data[0:split_idx]\n",
    "abnormal_ecg_test = abnormal_ecg_data[split_idx:]\n",
    "print(f'Abnormal ECG train shape: {abnormal_ecg_train.shape}')\n",
    "print(f'Abnormal ECG test shape: {abnormal_ecg_test.shape}')\n",
    "\n",
    "normal_metadata_train = normal_metadata[0:split_idx]\n",
    "normal_metadata_test = normal_metadata[split_idx:]\n",
    "print(f'Normal metadata train shape: {normal_metadata_train.shape}')\n",
    "print(f'Normal metadata test shape: {normal_metadata_test.shape}')\n",
    "\n",
    "abnormal_metadata_train = abnormal_metadata[0:split_idx]\n",
    "abnormal_metadata_test = abnormal_metadata[split_idx:]\n",
    "print(f'Abnormal metadata train shape: {abnormal_metadata_train.shape}')\n",
    "print(f'Abnormal metadata test shape: {abnormal_metadata_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize waveforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm:\n",
    "def normalize_waveform(data):\n",
    "    # Code generated from Bing Copilot\n",
    "    normalized_data = np.empty_like(data)\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[2]):\n",
    "            min_val = np.min(data[i, :, j])\n",
    "            max_val = np.max(data[i, :, j])\n",
    "\n",
    "            if max_val == min_val:\n",
    "                normalized_data[i, :, j] = 0\n",
    "            else:\n",
    "                normalized_data[i, :, j] = (data[i, :, j] - min_val) / (max_val - min_val)\n",
    "    return normalized_data\n",
    "\n",
    "# Since normalization occurs only within each record, there will be no contamination from train data\n",
    "std_normal_ecg_train = normalize_waveform(normal_ecg_train)\n",
    "std_normal_ecg_test = normalize_waveform(normal_ecg_test)\n",
    "\n",
    "std_abnormal_ecg_train = normalize_waveform(abnormal_ecg_train)\n",
    "std_abnormal_ecg_test = normalize_waveform(abnormal_ecg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine normal and abnormal data\n",
    "combined_ecg_train = np.concatenate((std_normal_ecg_train, std_abnormal_ecg_train), axis=0)\n",
    "combined_metadata_train = pd.concat([normal_metadata_train, abnormal_metadata_train], axis=0)\n",
    "combined_ecg_test = np.concatenate((std_normal_ecg_test, std_abnormal_ecg_test), axis=0)\n",
    "combined_metadata_test = pd.concat([normal_metadata_test, abnormal_metadata_test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode metadata and make DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers\n",
    "label_encoder = LabelEncoder()\n",
    "combined_metadata_train['diagnostic_superclass'] = label_encoder.fit_transform(combined_metadata_train['diagnostic_superclass'])\n",
    "combined_metadata_test['diagnostic_superclass'] = label_encoder.transform(combined_metadata_test['diagnostic_superclass'])\n",
    "\n",
    "\n",
    "scaler_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "encoder_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler_transformer, ['age']),\n",
    "        ('cat', encoder_transformer, ['sex', 'device', 'validated_by_human']),\n",
    "    ]\n",
    ")\n",
    "std_combined_metadata_train = preprocessor.fit_transform(combined_metadata_train.drop('diagnostic_superclass', axis=1)).toarray()\n",
    "std_combined_metadata_test = preprocessor.transform(combined_metadata_test.drop('diagnostic_superclass', axis=1)).toarray()\n",
    "\n",
    "batch_size = 32\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(torch.from_numpy(combined_ecg_train).float(),\n",
    "                              torch.from_numpy(std_combined_metadata_train).float(),\n",
    "                              torch.from_numpy(combined_metadata_train['diagnostic_superclass'].values).long())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.from_numpy(combined_ecg_test).float(),\n",
    "                             torch.from_numpy(std_combined_metadata_test).float(),\n",
    "                             torch.from_numpy(combined_metadata_test['diagnostic_superclass'].values).long())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tcn import TCN\n",
    "num_unique_devices = ptbxl_data['device'].nunique()\n",
    "class TCNAutoencoder(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, dropout, metadata_dims):\n",
    "        super(TCNAutoencoder, self).__init__()\n",
    "        self.encoder = TCN(\n",
    "            num_inputs=num_inputs,\n",
    "            num_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            causal=True,\n",
    "        )\n",
    "        self.age_embedding = nn.Linear(1, metadata_dims[0])  # Age is a single value\n",
    "        self.sex_embedding = nn.Linear(2, metadata_dims[1])  # Sex is one-hot encoded (2 columns)\n",
    "        self.device_embedding = nn.Linear(num_unique_devices, metadata_dims[2]) #one hot (11 cols)\n",
    "        self.validated_embedding = nn.Linear(2, metadata_dims[3]) #one hot (2 cols)\n",
    "        \n",
    "        decoder_input_dim = num_channels[-1] + sum(metadata_dims)\n",
    "        self.decoder = TCN(\n",
    "            num_inputs=decoder_input_dim,\n",
    "            num_channels=num_channels[::-1],\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,    \n",
    "            causal=True,\n",
    "            output_projection=num_inputs,\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, metadata):\n",
    "        encoded = self.encoder(x)\n",
    "        \n",
    "        age_emb = self.age_embedding(metadata[:, 0].unsqueeze(1))\n",
    "        sex_emb = self.sex_embedding(metadata[:, 1:3])\n",
    "        device_emb = self.device_embedding(metadata[:, 3:-2])\n",
    "        validated_emb = self.validated_embedding(metadata[:, -2:])\n",
    "        \n",
    "        metadata_emb = torch.cat([age_emb, sex_emb, device_emb, validated_emb], dim=-1)\n",
    "        metadata_emb = metadata_emb.unsqueeze(2).expand(-1, -1, encoded.size(2))\n",
    "        \n",
    "        concatenated = torch.cat([encoded, metadata_emb], dim=1)\n",
    "        decoded = self.decoder(concatenated)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNClassifier(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size, dropout, metadata_dims, num_classes):\n",
    "        super(TCNClassifier, self).__init__()\n",
    "        self.encoder = TCN(\n",
    "            num_inputs=num_inputs,\n",
    "            num_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            dropout=dropout,\n",
    "            causal=True,\n",
    "        )\n",
    "        self.age_embedding = nn.Linear(1, metadata_dims[0])\n",
    "        self.sex_embedding = nn.Linear(2, metadata_dims[1])\n",
    "        self.device_embedding = nn.Linear(num_unique_devices, metadata_dims[2])\n",
    "        self.validated_embedding = nn.Linear(2, metadata_dims[3])\n",
    "        \n",
    "        encoder_output_dim = num_channels[-1] + sum(metadata_dims)\n",
    "        #instead of a decoder use a sequential\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(encoder_output_dim, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, metadata):\n",
    "        encoded = self.encoder(x)\n",
    "        age_emb = self.age_embedding(metadata[:, 0].unsqueeze(1))\n",
    "        sex_emb = self.sex_embedding(metadata[:, 1:3])\n",
    "        device_emb = self.device_embedding(metadata[:, 3:-2])\n",
    "        validated_emb = self.validated_embedding(metadata[:, -2:])\n",
    "        metadata_emb = torch.cat([age_emb, sex_emb, device_emb, validated_emb], dim=-1)\n",
    "        metadata_emb = metadata_emb.unsqueeze(2).expand(-1, -1, encoded.size(2))\n",
    "        concatenated = torch.cat([encoded, metadata_emb], dim=1)\n",
    "        avg_pooled = torch.mean(concatenated, dim=2)\n",
    "        logits = self.classifier(avg_pooled)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: VERIFY AND ENSURE THIS IS CORRECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_inputs = 12  # Assuming 12 input channels in the ECG data\n",
    "num_channels = [32, 64, 128]  # Example number of channels in each residual block of the encoder\n",
    "kernel_size = 3  # Example kernel size for the TCN layers\n",
    "dropout = 0.2  # Example dropout rate\n",
    "metadata_dims = [10, 5, 20, 5]  # Example embedding dimensions for age, sex, and device, and validated\n",
    "\n",
    "pretrained_autoencoder = TCNAutoencoder(num_inputs, num_channels, kernel_size, dropout, metadata_dims)\n",
    "pretrained_autoencoder.load_state_dict(torch.load(\"./models/tcn.pth\"))\n",
    "\n",
    "model = TCNClassifier(num_inputs, num_channels, kernel_size, dropout, metadata_dims, 2)\n",
    "model.encoder.load_state_dict(pretrained_autoencoder.encoder.state_dict())\n",
    "\n",
    "# Freeze the encoder layers\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_data, batch_metadata, batch_labels in train_loader:\n",
    "        ecg_data = batch_data.to(device).permute(0, 2, 1).float()\n",
    "        batch_metadata = batch_metadata.to(device).float()\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ecg_data, batch_metadata)\n",
    "        loss = criterion(logits, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
